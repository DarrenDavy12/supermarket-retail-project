# Azure Databricks Medallion Pipeline: Retail Sales Data

## Project Overview

This project demonstrates an end-to-end **Medallion Architecture** pipeline on **Azure Databricks**, processing retail sales data from raw ingestion to clean and aggregated insights. The pipeline follows the Bronze â†’ Silver â†’ Gold layers to ensure clean, structured, and analysis-ready data.

---

## Architecture

`
Raw CSV (Bronze) -> Cleaned & Typed (Silver) -> Aggregated Metrics (Gold) -> Analysis / BI / Reporting
`

**Azure Services & Tools Used:**

- **Azure Databricks** â€“ Notebook environment and Spark cluster for ETL processing.
- **Azure Data Lake Storage Gen2 (ADLS)** â€“ Storage for Bronze, Silver, Gold layers.
- **Azure Key Vault** (planned) â€“ Secure management of credentials.
- **Azure Data Factory (ADF)** â€“ Orchestration and scheduling of pipeline workflows. (Couldn't ultilize orchestration/scheduling because of databricks tier choice, I made a conceptual approach in **ADF Architecture Diagram** section below.)
- **Parquet** â€“ Columnar storage format for Silver and Gold layers.
- **PySpark / Spark SQL** â€“ Data transformation and aggregation.
- **Databricks Notebooks** â€“ Organize and run ETL logic.
- **GitHub** â€“ Version control for notebooks and project files.
- **VS Code / IDE** â€“ Optional editing and debugging environment.
- **Python Libraries** â€“ Used within Databricks for additional transformations.

---

## Data Flow

1. ğŸ¥‰ **Bronze Layer (Raw)**
   - Ingest raw CSV file from ADLS.
   - Handle inconsistent formatting issues (e.g., different date formats).

2. ğŸ¥ˆ **Silver Layer (Cleaned)**
   - Apply schema enforcement and type conversion:
     - `Order Date` converted to proper timestamp.
     - `Sales`, `Profit`, `Discount` ensured to be numeric.
   - Resolve common data issues:
     - **Date parsing errors** (`CANNOT_PARSE_TIMESTAMP`) handled using `to_date` with proper format.
     - Ignored invalid or inconsistent rows.
   - Store cleaned data in **Parquet** format in the Silver container.

3. ğŸ¥‡ **Gold Layer (Aggregated)**
   - Aggregate metrics by `Region`, `Category`, or `State`.
   - Produce analysis-ready datasets for BI or reporting.
   - Store in **Parquet** format in the Gold container.

4. **Pipeline Orchestration**
   - Use **Azure Data Factory** to schedule and automate the pipeline.
   - Orchestrate Bronze â†’ Silver â†’ Gold processing.
   - Integrate Databricks notebooks as pipeline activities.

---

## Challenges/Errors Resolved

During development, the following issues were encountered and resolved:

1. **File format issues**
   - Error: `FAILED_READ_FILE.CANNOT_READ_FILE_FOOTER`
   - Cause: Trying to read CSV as Parquet.
   - Solution: Ensure proper file format (`spark.read.format("csv")` for raw files, Parquet for processed layers).

2. **Timestamp parsing errors**
   - Error: `CANNOT_PARSE_TIMESTAMP`
   - Cause: Mixed date formats in the CSV (`MM-dd-yyyy` vs `M/d/yyyy`).
   - Solution: Used PySpark `to_date()` with correct `format` parameter and nullable handling.

3. **Column resolution errors**
   - Error: `column1 cannot be resolved`
   - Cause: Column names referenced did not exist.
   - Solution: Checked schema and referenced correct column names (e.g., `City`, `Region`, `Sales`).

4. **Storage access errors**
   - Error: `KeyProviderException` / `Invalid configuration value for fs.azure.account.key`
   - Cause: Hard-coded or missing ADLS credentials.
   - Solution: Future improvement: use **Azure Key Vault** for secret management instead of hard-coded keys.

---

## Usage

1. **Setup**
   - Configure Databricks cluster.
   - Ensure access to ADLS containers: `bronze`, `silver`, `gold`.

2. **Run Notebooks**
   - `bronze_silver.ipynb` â†’ ingests raw CSV and writes to Silver layer.
   - `gold.ipynb` â†’ reads Silver layer and produces Gold layer.

3. **Verify Output**
   - Silver layer: Cleaned Parquet files.
   - Gold layer: Aggregated metrics in Parquet.

4. **Pipeline Automation (Optional)**
   - Use **ADF** to schedule notebooks as activities.
   - Monitor pipeline runs and failures via ADF.

    ADF Pipeline: Supermarket Sales ETL

    Pipeline steps:

    1. Bronze â†’ Silver notebook (Databricks)
    - Reads raw CSV from Bronze layer
    - Cleans data and writes to Silver layer

    2. Silver â†’ Gold notebook (Databricks)
    - Reads Silver layer
    - Aggregates and writes Gold layer

    Activities are connected via Success dependency (Gold runs after Silver completes)

    **ADF Architecture Diagram**

                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚  Bronze CSV â”‚
                 â”‚  Raw Data   â”‚
                 â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Bronze â†’ Silver Notebookâ”‚
         â”‚ Databricks PySpark      â”‚
         â”‚ - Cleans dates          â”‚
         â”‚ - Removes nulls         â”‚
         â”‚ - Standardizes columns  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Silver Layerâ”‚
            â”‚ (Parquet)   â”‚
            â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Silver â†’ Gold       â”‚
         â”‚ Notebook (Databricks)â”‚
         â”‚ - Aggregations       â”‚
         â”‚ - Enrichments        â”‚
         â”‚ - Final prep         â”‚
         â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Gold Layerâ”‚
          â”‚ (Parquet) â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

---

## Future Improvements

- Integrate **Azure Key Vault** to store and retrieve ADLS credentials securely.
- Expand **ADF orchestration** for error handling and notifications.
- Add **BI dashboards** on top of Gold layer data.
- Implement **data quality checks** at each Medallion stage.

---
 
**Project Status:** Complete Bronze â†’ Silver â†’ Gold pipeline with cleaned and aggregated datasets, ready for orchestration via ADF.  
**Tools & Services Used:** Azure Databricks, ADLS Gen2, Azure Data Factory, PySpark, Python, Spark SQL, Parquet, Azure Key Vault (planned, couuld not apply because of databricks tier 14-day-trial doesnt allow secrets/scope), GitHub, VS Code (optional, I did for project structure).
